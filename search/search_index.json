{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview This tutorial provides a step-by-step guide to performing basic polygenic risk score (PRS) analyses and accompanies our PRS Guide paper . Our hope is that this tutorial will allow new users to get started on PRS analyses and provide existing users with a better understanding of the processes and implemention underlying popular PRS software. The tutorial is separated into four main sections: QC of Base GWAS Summary Data QC of Target Individual-Level Data Running PRS Analyses Visualizing PRS Results We provide brief examples of performing PRS analyses using four software for polygenic risk score analyses: PLINK , PRSice-2 , LDpred and lassosum If you are only interested in how to perform PRS on previously QC'ed data then you can directly skip to step 3 . Links to download the required data are provided under each section. Note This tutorial is written for Linux and OS X operating systems. Windows users will need to change some commands accordingly. Note Throughout the tutorial you will see tabs above some of the code: A echo Tab A B echo Tab B You can click on the tab to change to an alternative code (eg. to a different operation system) Datasets Base data : Modified summary statistic file from GIANT Target data : Simulated data based on 1000 genome European samples Requirements To follow the tutorial, you will need the following programs installed: R ( version 3.2.3+ ) PLINK 1.9 Citation If you find this tutorial helpful for a publication, then please consider citing: Citation Choi SW, Mak TSH, O'Reilly PF. A guide to performing Polygenic Risk Score analyses. bioRxiv 416545 (2018). https://doi.org/10.1101/416545","title":"Overview"},{"location":"#overview","text":"This tutorial provides a step-by-step guide to performing basic polygenic risk score (PRS) analyses and accompanies our PRS Guide paper . Our hope is that this tutorial will allow new users to get started on PRS analyses and provide existing users with a better understanding of the processes and implemention underlying popular PRS software. The tutorial is separated into four main sections: QC of Base GWAS Summary Data QC of Target Individual-Level Data Running PRS Analyses Visualizing PRS Results We provide brief examples of performing PRS analyses using four software for polygenic risk score analyses: PLINK , PRSice-2 , LDpred and lassosum If you are only interested in how to perform PRS on previously QC'ed data then you can directly skip to step 3 . Links to download the required data are provided under each section. Note This tutorial is written for Linux and OS X operating systems. Windows users will need to change some commands accordingly. Note Throughout the tutorial you will see tabs above some of the code: A echo Tab A B echo Tab B You can click on the tab to change to an alternative code (eg. to a different operation system)","title":"Overview"},{"location":"#datasets","text":"Base data : Modified summary statistic file from GIANT Target data : Simulated data based on 1000 genome European samples","title":"Datasets"},{"location":"#requirements","text":"To follow the tutorial, you will need the following programs installed: R ( version 3.2.3+ ) PLINK 1.9","title":"Requirements"},{"location":"#citation","text":"If you find this tutorial helpful for a publication, then please consider citing: Citation Choi SW, Mak TSH, O'Reilly PF. A guide to performing Polygenic Risk Score analyses. bioRxiv 416545 (2018). https://doi.org/10.1101/416545","title":"Citation"},{"location":"base/","text":"First step in Polygenic Risk Score (PRS) analyses is to obtain the GWAS summary statistics. In this example, we will use a modified version of the Height GWAS summary statistics generated by the GIANT consortium Obtaining the summary statistic file You can download the summary statistic file here or use the following script: curl https://github.com/choishingwan/PRS-Tutorial/raw/master/resources/GIANT.height.gz -L -O which will create a file call GIANT.height.gz in your current directory. A common problem is that the downloaded file can be corrupted, which can generate various error messages in the down-stream analyses. To avoid un-necessary waste of time, it is generally a good practice to check if the file is intact. If the md5sum hash is provided for the original file, you can check the file integrity by performing the md5sum check: Linux md5sum GIANT.height.gz OS X md5 GIANT.height.gz if the file is intact, md5sum should generate a string of characters: c79734b099cea663d2808bfde2e9a422 . If a different string is generated, the file is corrupted Reading the summary statistic file GIANT.height.gz is compressed. To read its content, you can type: gunzip -c GIANT.height.gz | head which will shows the first 10 lines of the file Note Working with compressed files reduces the storage space requirements The GIANT.height.gz file contains the following columns: SNP CHR BP A1 A2 MAF SE P N INFO OR With each column corresponds to the following SNP : SNP ID, usually in the form of RS-ID CHR : The chromosome of which the SNP resides on BP : Chromosomal coordinate of the SNP A1 : The effective alllele of the SNP A2 : The non-effective allele of the SNP MAF : The minor allele frequency of the SNP SE : The standard error of the effect size esimate P : The P-value of association between the genotype of the SNP and the phenotype of interest N : Number of samples used to obtain the effect size estimate INFO : Usually the imputation information score. OR : The effect size estimate of the SNP. Can also be BETA Important Some GWAS results files do not make clear which allele is the effect allele and which the non-effect allele. If the incorrect assumption is made in computing the PRS, then the effect of the PRS in the target data will be in the wrong direction. To avoid misleading conclusions it is critical that the effect allele from the base (GWAS) data is known. Removing duplicated SNPs While it is rare, duplciated SNPs can sometimes be found in your base file. Most PRS software do not allow duplicated SNPs in the base input, therefore it is beneficial to remove the duplicated SNPs from the base file. gunzip -c GIANT.height.gz | \\ awk { print $1} | \\ sort | \\ uniq -d duplicated.snp Briefly, the above command does the following: Decompress and read the GIANT.height.gz file Print out the first column of the file (which contains the SNP ID, change $1 to other number if the SNP ID is located in another column, e.g. $3 if the SNP ID is located on the third column) Sort the SNP IDs. This will put duplicated SNP IDs next to eachother Print out any duplicated SNP IDs using the uniq command and print it to the duplicated.snp file How many duplicated SNPs are there? There are a total of 13 duplicated SNPs Duplicated SNPs can then be removed using the grep command: gunzip -c GIANT.height.gz | \\ grep -vf duplicated.snp | \\ gzip - Height.gz The above script does the following: Decompress and read the GIANT.height.gz file Find if any row contains entries observed in duplicated.snp and remove them Compress and write the results to Height.gz Filtering SNPs with low INFO score or MAF SNPs with low minor allele frequency (MAF) or imputation information score (INFO) are more likely to harbor false positives. It is therefore beneficial to remove SNPs with low MAF and INFO. This can be acheived using the following code: gunzip -c Height.gz | \\ awk NR==1 || ($6 0.05 $6 0.95) ($10 0.8) {print} | \\ gzip Height.QC.gz Decompress and read the Height.gz file Print the header line ( NR==1 ) Print any line with MAF above 0.05 and less than 0.95 ( $6 because the sixth column of the file contains the MAF information) Print any line with INFO above 0.8 ( $10 because the tenth column of the file contains the INFO information) Compress and write the result to Height.QC.gz The Height.QC.gz file can then be used for downstream analyses","title":"1. QC of Summary Statistics"},{"location":"base/#obtaining-the-summary-statistic-file","text":"You can download the summary statistic file here or use the following script: curl https://github.com/choishingwan/PRS-Tutorial/raw/master/resources/GIANT.height.gz -L -O which will create a file call GIANT.height.gz in your current directory. A common problem is that the downloaded file can be corrupted, which can generate various error messages in the down-stream analyses. To avoid un-necessary waste of time, it is generally a good practice to check if the file is intact. If the md5sum hash is provided for the original file, you can check the file integrity by performing the md5sum check: Linux md5sum GIANT.height.gz OS X md5 GIANT.height.gz if the file is intact, md5sum should generate a string of characters: c79734b099cea663d2808bfde2e9a422 . If a different string is generated, the file is corrupted","title":"Obtaining the summary statistic file"},{"location":"base/#reading-the-summary-statistic-file","text":"GIANT.height.gz is compressed. To read its content, you can type: gunzip -c GIANT.height.gz | head which will shows the first 10 lines of the file Note Working with compressed files reduces the storage space requirements The GIANT.height.gz file contains the following columns: SNP CHR BP A1 A2 MAF SE P N INFO OR With each column corresponds to the following SNP : SNP ID, usually in the form of RS-ID CHR : The chromosome of which the SNP resides on BP : Chromosomal coordinate of the SNP A1 : The effective alllele of the SNP A2 : The non-effective allele of the SNP MAF : The minor allele frequency of the SNP SE : The standard error of the effect size esimate P : The P-value of association between the genotype of the SNP and the phenotype of interest N : Number of samples used to obtain the effect size estimate INFO : Usually the imputation information score. OR : The effect size estimate of the SNP. Can also be BETA Important Some GWAS results files do not make clear which allele is the effect allele and which the non-effect allele. If the incorrect assumption is made in computing the PRS, then the effect of the PRS in the target data will be in the wrong direction. To avoid misleading conclusions it is critical that the effect allele from the base (GWAS) data is known.","title":"Reading the summary statistic file"},{"location":"base/#removing-duplicated-snps","text":"While it is rare, duplciated SNPs can sometimes be found in your base file. Most PRS software do not allow duplicated SNPs in the base input, therefore it is beneficial to remove the duplicated SNPs from the base file. gunzip -c GIANT.height.gz | \\ awk { print $1} | \\ sort | \\ uniq -d duplicated.snp Briefly, the above command does the following: Decompress and read the GIANT.height.gz file Print out the first column of the file (which contains the SNP ID, change $1 to other number if the SNP ID is located in another column, e.g. $3 if the SNP ID is located on the third column) Sort the SNP IDs. This will put duplicated SNP IDs next to eachother Print out any duplicated SNP IDs using the uniq command and print it to the duplicated.snp file How many duplicated SNPs are there? There are a total of 13 duplicated SNPs Duplicated SNPs can then be removed using the grep command: gunzip -c GIANT.height.gz | \\ grep -vf duplicated.snp | \\ gzip - Height.gz The above script does the following: Decompress and read the GIANT.height.gz file Find if any row contains entries observed in duplicated.snp and remove them Compress and write the results to Height.gz","title":"Removing duplicated SNPs"},{"location":"base/#filtering-snps-with-low-info-score-or-maf","text":"SNPs with low minor allele frequency (MAF) or imputation information score (INFO) are more likely to harbor false positives. It is therefore beneficial to remove SNPs with low MAF and INFO. This can be acheived using the following code: gunzip -c Height.gz | \\ awk NR==1 || ($6 0.05 $6 0.95) ($10 0.8) {print} | \\ gzip Height.QC.gz Decompress and read the Height.gz file Print the header line ( NR==1 ) Print any line with MAF above 0.05 and less than 0.95 ( $6 because the sixth column of the file contains the MAF information) Print any line with INFO above 0.8 ( $10 because the tenth column of the file contains the INFO information) Compress and write the result to Height.QC.gz The Height.QC.gz file can then be used for downstream analyses","title":"Filtering SNPs with low INFO score or MAF"},{"location":"lassosum/","text":"lassosum is an R package for PRS calculation. It uses LASSO/Elastic Net estimates rather than p-value thresholding to generate PRS and are expected to provide a higher \\(R^2\\) when compared to p-value thresholding. You can install lassosum and its dependencies in R with the following install.packages ( c ( devtools , RcppArmadillo , data.table , Matrix ), dependencies = TRUE ) library ( devtools ) install_github ( tshmak/lassosum ) Assuming we have the following files File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.valid.sample This file contains the samples that passed all the QC EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Running PRS analysis We can run lassosum as follow library ( lassosum ) # Prefer to work with data.table as it speeds up file reading library ( data.table ) library ( methods ) # We like to use dplyr for it makes codes much more readable library ( dplyr ) sum.stat - Height.QC.gz bfile - EUR.QC # Read in and process the covariates covariate - fread ( EUR.covariate ) pcs - fread ( EUR.eigenvec ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # Need as.data.frame here as lassosum doesn t handle data.table # covariates very well cov - as.data.frame ( merge ( covariate , pcs , by = c ( FID , IID ))) # We will need the EUR.hg19 file provided by lassosum # which are LD regions defined in Berisa and Pickrell (2015) for the European population and the hg19 genome. ld.file - system.file ( data , Berisa.EUR.hg19.bed , package = lassosum ) # output prefix prefix - EUR # Read in the target phenotype file target.pheno - as.data.frame ( fread ( EUR.height )[, c ( FID , IID , Height )]) # Read in samples to include in the analysis target.keep - fread ( EUR.valid.sample )[, c ( FID , IID )] # Read in the summary statistics ss - fread ( sum.stat ) # Number of sample in base size - 253288 # Remove P-value = 0, which causes problem in the transformation ss - ss [ ! P == 0 ] # Read in the LD blocks ld - fread ( ld.file ) # Transform the P-values into correlation cor - p2cor ( p = ss $ P , n = size , sign = log ( ss $ OR ) ) # Because FID of our samples are all 0, we might encounter problem with lassosum # we need to provide a T/F vector instead of the target.keep file target.keep [, ID := do.call ( paste , c ( . SD , sep = : )), . SDcols = c ( 1 : 2 )] fam - fread ( paste0 ( bfile , .fam )) fam [, ID := do.call ( paste , c ( . SD , sep = : )), . SDcols = c ( 1 : 2 )] keep - fam $ ID %in% target.keep $ ID # Run the lassosum pipeline out - lassosum.pipeline ( cor = cor , chr = ss $ CHR , pos = ss $ BP , A1 = ss $ A1 , A2 = ss $ A2 , ref.bfile = bfile , keep.ref = keep , test.bfile = bfile , keep.test = keep , LDblocks = ld , trace = 2 ) # Store the R2 results target.res - validate ( out , pheno = target.pheno , covar = cov ) # Get the maximum R2 r2 - max ( target.res $ validation.table $ value ) ^ 2","title":"lassosum"},{"location":"lassosum/#running-prs-analysis","text":"We can run lassosum as follow library ( lassosum ) # Prefer to work with data.table as it speeds up file reading library ( data.table ) library ( methods ) # We like to use dplyr for it makes codes much more readable library ( dplyr ) sum.stat - Height.QC.gz bfile - EUR.QC # Read in and process the covariates covariate - fread ( EUR.covariate ) pcs - fread ( EUR.eigenvec ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # Need as.data.frame here as lassosum doesn t handle data.table # covariates very well cov - as.data.frame ( merge ( covariate , pcs , by = c ( FID , IID ))) # We will need the EUR.hg19 file provided by lassosum # which are LD regions defined in Berisa and Pickrell (2015) for the European population and the hg19 genome. ld.file - system.file ( data , Berisa.EUR.hg19.bed , package = lassosum ) # output prefix prefix - EUR # Read in the target phenotype file target.pheno - as.data.frame ( fread ( EUR.height )[, c ( FID , IID , Height )]) # Read in samples to include in the analysis target.keep - fread ( EUR.valid.sample )[, c ( FID , IID )] # Read in the summary statistics ss - fread ( sum.stat ) # Number of sample in base size - 253288 # Remove P-value = 0, which causes problem in the transformation ss - ss [ ! P == 0 ] # Read in the LD blocks ld - fread ( ld.file ) # Transform the P-values into correlation cor - p2cor ( p = ss $ P , n = size , sign = log ( ss $ OR ) ) # Because FID of our samples are all 0, we might encounter problem with lassosum # we need to provide a T/F vector instead of the target.keep file target.keep [, ID := do.call ( paste , c ( . SD , sep = : )), . SDcols = c ( 1 : 2 )] fam - fread ( paste0 ( bfile , .fam )) fam [, ID := do.call ( paste , c ( . SD , sep = : )), . SDcols = c ( 1 : 2 )] keep - fam $ ID %in% target.keep $ ID # Run the lassosum pipeline out - lassosum.pipeline ( cor = cor , chr = ss $ CHR , pos = ss $ BP , A1 = ss $ A1 , A2 = ss $ A2 , ref.bfile = bfile , keep.ref = keep , test.bfile = bfile , keep.test = keep , LDblocks = ld , trace = 2 ) # Store the R2 results target.res - validate ( out , pheno = target.pheno , covar = cov ) # Get the maximum R2 r2 - max ( target.res $ validation.table $ value ) ^ 2","title":"Running PRS analysis"},{"location":"ldpred/","text":"Another popular PRS software is LDpred , which instead of performing p-value thresholding, infers the posterior mean effect size of each marker by using a prior on effect sizes and LD information from an external reference panel, thus allow for a better \\(R^2\\) . Note Python 3 and other packages need to be installed before running LDpred. Please refer to their website for instructions of installation. If you have Python installed, you might be able to install LDpred with pip install ldpred Note Current script is based on version 1.0.6 Assuming we have the following files File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.valid.sample This file contains the samples that passed all the QC EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Running PRS analysis LDpred does not support in place filtering of sample and SNPs, therefore we need to generate a new QCed genotype file using plink # We also add the height phenotype for convenience plink \\ --bfile EUR.QC \\ --pheno EUR.height \\ --keep EUR.valid.sample \\ --make-bed \\ --out EUR.ldpred LDpred can then performs PRS analysis in three steps Preprocessing the summary statistic file # There are 253,288 samples in the Height GWAS python LDpred.py coord \\ --rs SNP \\ --A1 A1 \\ --A2 A2 \\ --pos BP \\ --chr CHR \\ --pval P \\ --eff OR \\ --ssf-format CUSTOM \\ --N 253288 \\ --ssf Height.QC.gz \\ --out EUR.coord \\ --gf EUR.ldpred Adjust the effect size # LDpred recommend radius to be Total number of SNPs in target / 3000 python LDpred.py gibbs \\ --cf EUR.coord \\ --ldr 183 \\ --ldf EUR.ld \\ --out EUR.weight \\ --N 253288 Calculate the PRS python LDpred.py score \\ --gf EUR.ldpred \\ --rf EUR.weight \\ --out EUR.score \\ --pf EUR.height \\ --pf-format LSTANDARD Note To obtain the \\(R^2\\) of PRS obtained from different parameters, and / or adjust for covariate, you might need to use R (see here )","title":"LDpred"},{"location":"ldpred/#running-prs-analysis","text":"LDpred does not support in place filtering of sample and SNPs, therefore we need to generate a new QCed genotype file using plink # We also add the height phenotype for convenience plink \\ --bfile EUR.QC \\ --pheno EUR.height \\ --keep EUR.valid.sample \\ --make-bed \\ --out EUR.ldpred LDpred can then performs PRS analysis in three steps Preprocessing the summary statistic file # There are 253,288 samples in the Height GWAS python LDpred.py coord \\ --rs SNP \\ --A1 A1 \\ --A2 A2 \\ --pos BP \\ --chr CHR \\ --pval P \\ --eff OR \\ --ssf-format CUSTOM \\ --N 253288 \\ --ssf Height.QC.gz \\ --out EUR.coord \\ --gf EUR.ldpred Adjust the effect size # LDpred recommend radius to be Total number of SNPs in target / 3000 python LDpred.py gibbs \\ --cf EUR.coord \\ --ldr 183 \\ --ldf EUR.ld \\ --out EUR.weight \\ --N 253288 Calculate the PRS python LDpred.py score \\ --gf EUR.ldpred \\ --rf EUR.weight \\ --out EUR.score \\ --pf EUR.height \\ --pf-format LSTANDARD Note To obtain the \\(R^2\\) of PRS obtained from different parameters, and / or adjust for covariate, you might need to use R (see here )","title":"Running PRS analysis"},{"location":"plink/","text":"In previous sections, we have generated the following files File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.valid.sample This file contains the samples that passed all the QC EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples Here, we will try to calculate polygenic risk score using plink . Remove Ambiguous SNPs If the base and target data were generated using different genotyping chips and the chromosome strand (+/-) for either is unknown, then it is not possible to match ambiguous SNPs (i.e. those with complementary alleles, either C/G or A/T) across the data sets, because it will be unknown whether the base and target data are referring to the same allele or not. Ambiguous SNPs can be obtained by examining the bim file: awk !( ($5== A $6== T ) || \\ ($5== T $6== A ) || \\ ($5== G $6== C ) || \\ ($5== C $6== G )) {print} \\ EUR.QC.bim EUR.unambig.snp How many ambiguous SNPs were there? There are 17,260 ambiguous SNPs Strand Flipping In addition, when there are non-ambiguous mismatch in allele coding between the data sets, such as A/C in the base and G/T in the target data, then this can be resolved by \u2018flipping\u2019 the alleles in the target data to their complementary alleles. This has to be done with mulitpe steps Get the correct A1 alleles for the bim file Without data.table bim - read.table ( EUR.QC.bim , header = F , stringsAsFactors = F ) colnames ( bim ) - c ( CHR , SNP , CM , BP , B.A1 , B.A2 ) height - read.table ( gzfile ( Height.QC.gz ), header = T , stringsAsFactors = F ) # Change all alleles to upper case for easy comparison height $ A1 - toupper ( height $ A1 ) height $ A2 - toupper ( height $ A2 ) bim $ B.A1 - toupper ( bim $ B.A1 ) bim $ B.A2 - toupper ( bim $ B.A2 ) info - merge ( bim , height , by = c ( SNP , CHR , BP )) # Function for finding the complementary allele complement - function ( x ) { switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Get SNPs that has the same alleles across base and target info.match - subset ( info , A1 == B.A1 A2 == B.A2 ) # Identify SNPs that are complementary between base and target info $ C.A1 - sapply ( info $ B.A1 , complement ) info $ C.A2 - sapply ( info $ B.A2 , complement ) info.complement - subset ( info , A1 == C.A1 A2 == C.A2 ) # Update these allele coding in the bim file bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A1 - sapply ( bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A1 , complement ) bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A2 - sapply ( bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A2 , complement ) # identify SNPs that need flipping info.flip - subset ( info , A1 == B.A2 A2 == B.A1 ) # identify SNPs that need flipping complement info.cflip - subset ( info , A1 == C.A2 A2 == C.A1 ) # Update these allele coding in the bim file com.snps - bim $ SNP %in% info.cflip $ SNP bim [ com.snps ,] $ B.A1 - sapply ( bim [ com.snps ,] $ B.A1 , complement ) bim [ com.snps ,] $ B.A2 - sapply ( bim [ com.snps ,] $ B.A2 , complement ) # Get list of SNPs that need to change the A1 encoding flip - rbind ( info.flip , info.cflip ) flip.snp - data.frame ( SNP = flip $ SNP , A1 = flip $ A1 ) write.table ( flip.snp , EUR.update.a1 , quote = F , row.names = F ) write.table ( bim , EUR.QC.adj.bim , quote = F , row.names = F , col.names = F ) # And we want to remove any SNPs that do not match with the base data mismatch - bim $ SNP [ ! ( bim $ SNP %in% info.match $ SNP | bim $ SNP %in% info.complement $ SNP | bim $ SNP %in% flip $ SNP )] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) With data.table library ( data.table ) bim - fread ( EUR.QC.bim ) bim.col - c ( CHR , SNP , CM , BP , B.A1 , B.A2 ) setnames ( bim , colnames ( bim ), bim.col ) height - fread ( Height.QC.gz ) # Change all alleles to upper case for easy comparison height [, c ( A1 , A2 ) := list ( toupper ( A1 ), toupper ( A2 ))] bim [, c ( B.A1 , B.A2 ) := list ( toupper ( B.A1 ), toupper ( B.A2 ))] info - merge ( bim , height , by = c ( SNP , CHR , BP )) # Function for calculating the complementary allele complement - function ( x ){ switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Identify SNPs that are complementary between base and target com.snps - info [ sapply ( B.A1 , complement ) == A1 sapply ( B.A2 , complement ) == A2 , SNP ] # Now update the bim file bim [ SNP %in% com.snps , c ( B.A1 , B.A2 ) := list ( sapply ( B.A1 , complement ), sapply ( B.A2 , complement ))] # identify SNPs that need flipping complement com.flip - info [ sapply ( B.A1 , complement ) == A2 sapply ( B.A2 , complement ) == A1 , SNP ] # Now update the bim file bim [ SNP %in% com.flip , c ( B.A1 , B.A2 ) := list ( sapply ( B.A1 , complement ), sapply ( B.A2 , complement ))] # Obtain list of SNPs that require flipping flip - info [ B.A1 == A2 B.A2 == A1 ] # Now generate file for PLINK fwrite ( flip [, c ( SNP , A1 )], EUR.update.a1 , sep = \\t ) # Write the updated bim file fwrite ( bim , EUR.QC.adj.bim , col.names = F , sep = \\t ) # We can then remove all mismatch SNPs matched - info [( A1 == B.A1 A2 == B.A2 ) | ( A1 == B.A2 A2 == B.A1 ) | ( A1 == sapply ( B.A1 , complement ) A2 == sapply ( B.A2 , complement )) | ( A1 == sapply ( B.A2 , complement ) A2 == sapply ( B.A1 , complement ))] mismatch - bim [ ! SNP %in% matched $ SNP , SNP ] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) The above script will generate three files: EUR.QC.adj.bim , EUR.update.a1 and EUR.mismatch . We want to replace EUR.QC.bim with EUR.QC.adj.bim : # Make a back up mv EUR.QC.bim EUR.QC.bim.bk ln -s EUR.QC.adj.bim EUR.QC.bim We can then generate a new genotype file with the correct genetic encodings plink \\ --bfile EUR.QC \\ --a1-allele EUR.update.a1 \\ --make-bed \\ --keep EUR.valid.sample \\ --extract EUR.unambig.snp \\ --exclude EUR.mismatch \\ --out EUR.QC.flipped Update Effect Size When odd ratios (OR) instead of BETA are provided, the PRS might have to calculated using a multiplicative model. To simplify the calculation, we usually take the natural logarithm of the OR such that an additive model can be applied. We can obtain the transformed summary statistics with R : Without data.table dat - read.table ( gzfile ( Height.QC.gz ), header = T ) dat $ OR - log ( dat $ OR ) write.table ( dat , Height.QC.Transformed , quote = F , row.names = F ) With data.table library ( data.table ) dat - fread ( Height.QC.gz ) fwrite ( dat [, OR := log ( OR )], Height.QC.Transformed , sep = \\t ) Warning It might be tempting to perform the log transofrmation using awk . However, due to a lower arithmetic precision of awk , less accurate results might be obtained. Therefore it is best to do the transformation in R or allow the PRS software to perform the transformation directly. Clumping Linkage disequilibrium introduce a strong correlation structure across the genome, makes identifying the independent genetic effects extremely challenging. One simple method is to perform clumping, which preferentially selects SNPs most associated with the trait under study when removing SNPs in LD. plink \\ --bfile EUR.QC.flipped \\ --clump-p1 1 \\ --clump-r2 0 .1 \\ --clump-kb 250 \\ --clump Height.QC.transformed \\ --clump-snp-field SNP \\ --clump-field P \\ --out EUR Each of the new parameters corresponds to the following Paramter Value Description clump-p1 1 P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping clump-r2 0.1 SNPs having \\(r^2\\) higher than 0.1 with the index SNPs will be removed clump-kb 250 SNPs within 250k of the index SNP are considered for clumping clump Height.QC.transformed Summary statistic file containing the p-value information clump-snp-field SNP Specify that the column SNP contains the SNP IDs clump-field P Specify that the column P contains the P-value information A more detailed document can be found here Note The \\(r^2\\) values computed by --clump are based on maximum likelihood haplotype frequency estimates This will generate EUR.clumped , containing the index SNPs after clumping is performed. We can extract the index SNP ID by doing awk NR!=1{print $3} EUR.clumped EUR.valid.snp $3 because the third column contains the SNP ID Note If your target sample is small (e.g. 500), you can try using the 1000 Genome samples for the LD calculation. Make sure you use the population that best represents your sample. Generate PRS plink provide a handy function --score and --q-score-range for calculating polygenic score. We will need three files The summary statistic file: Height.QC.Transformed A file containing SNP ID and their corresponding p-value ( $1 because SNP ID is located at the first column; $8 because P-value is located at the eigth column) awk {print $1,$8} Height.QC.Transformed SNP.pvalue A file containing the P-value thresholds we are testing. Here we will only test a few for illustration purposes echo 0.001 0 0.001 range_list echo 0.05 0 0.05 range_list echo 0.1 0 0.1 range_list echo 0.2 0 0.2 range_list echo 0.3 0 0.3 range_list echo 0.4 0 0.4 range_list echo 0.5 0 0.5 range_list The format of the range_list file should be as follow Name of Threshold Lower bound Upper Bound Note The boundary are inclusive. For example, for the 0.05 threshold, we include all SNPs with P-value from 0 to 0.05 , including any SNPs with P-value equal to 0.05 We can then calculate the PRS with the following plink command: plink \\ --bfile EUR.QC.flipped \\ --extract EUR.valid.snp \\ --score Height.QC.Transformed 1 4 11 header \\ --q-score-range range_list SNP.pvalue \\ --out EUR Meaning of the new parameters are as follow Paramter Value Description score Height.QC.Transformed 1 4 11 header We read from the Height.QC.Transformed file, assuming the 1 st column to be the SNP ID; 4 th column to be the effective allele information; 11 th column to be the effect size estimate; and the file contains a header q-score-range range_test SNP.pvalue We want to calculate PRS based on the thresholds defined in range_test , where the threshold values (p-values) were stored in SNP.pvalue The above command and range_list will generate 7 files: EUR.0.5.profile EUR.0.4.profile EUR.0.3.profile EUR.0.2.profile EUR.0.1.profile EUR.0.05.profile EUR.0.001.profile Note The default formular for PRS calculation in PLINK is: (Assuming the effect size of SNP \\(i\\) is \\(S_i\\) ; the number of effective allele observed in sample \\(j\\) is \\(G_{ij}\\) ; the ploidy of the sample is \\(P\\) (It should be 2 for human); the number of samples included in the PRS be \\(N\\) ; and the number of non-missing SNPs observed in sample \\(j\\) be \\(M_j\\) ) $$ PRS_j =\\frac{ \\sum_i^NS_i*G_{ij}}{P*M_j} $$ If sample has a missing genotype for SNP \\(i\\) , the population minor allele frequency times ploidy ( \\(MAF_i*P\\) ) is used inplace of \\(G_{ij}\\) Accounting for Population Stratification Population structure is the principal source of confounding in GWAS and are usually accounted for by incorporating the principal components (PCs) as a covariate. Similarly, we can incorporate PCs in our PRS analysis to account for population stratification. Again, we can calculate the PCs using plink # First, we need to perform prunning plink \\ --bfile EUR.QC.flipped \\ --extract EUR.valid.snp \\ --indep-pairwise 200 50 0 .25 \\ --out EUR # Then we calculate the first 6 PCs plink \\ --bfile EUR.QC.flipped \\ --extract EUR.prune.in \\ --pca 6 \\ --out EUR Note One way to select the appropriate number of PCs is to perform GWAS on the trait of interest with different number of PCs. LDSC analysis can then be performed on each of the resulted GWAS summary statistics. By observing the estimated intercept, one can select the number of PCs that provide an intercept estimate closer to 1, which might suggest a smaller influence of population stratification. The eigen-vector (PCs) are stored in EUR.eigenvec and can be used as a covariate in the regression model to account for population stratification. Important If the base and target samples are collected from different population (e.g. Caucasian vs African ), the results from PRS analysis will be biased (see Martin et al ). Finding the \"Best\" P-value threshold The \"best\" p-value threshold for PRS construction are usually not known. To identify the \"best\" PRS, we can perform a regression between the calculated PRS and the sample phenotype and select the PRS that explains most of the phenotypic variation. This can be achieved using R . detail p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) # Read in the phenotype file phenotype - read.table ( EUR.height , header = T ) # Read in the PCs pcs - read.table ( EUR.eigenvec , header = F ) # The default output from plink does not include a header # To make things simple, we will add the appropriate headers # (1:6 because there are 6 PCs) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # Read in the covariates (here, it is sex) covariate - read.table ( EUR.covariate , header = T ) # Now merge the files pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) # We can then calculate the null model (model with PRS) using a linear regression # (as height is quantitative) null.model - lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )]) # And the R2 of the null model is null.r2 - summary ( null.model ) $ r.squared prs.result - NULL for ( i in p.threshold ){ # Go through each p-value threshold prs - read.table ( paste0 ( EUR. , i , .profile ), header = T ) # Merge the prs with the phenotype matrix # We only want the FID, IID and PRS from the PRS file, therefore we only select the # relevant columns pheno.prs - merge ( pheno , prs [, c ( FID , IID , SCORE )], by = c ( FID , IID )) # Now perform a linear regression on Height with PRS and the covariates # ignoring the FID and IID from our model model - lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )]) # model R2 is obtained as model.r2 - summary ( model ) $ r.squared # R2 of PRS is simply calculated as the model R2 minus the null R2 prs.r2 - model.r2 - null.r2 # We can also obtain the coeffcient and p-value of association of PRS as follow prs.coef - summary ( model ) $ coeff [ SCORE ,] prs.beta - as.numeric ( prs.coef [ 1 ]) prs.se - as.numeric ( prs.coef [ 2 ]) prs.p - as.numeric ( prs.coef [ 4 ]) # We can then store the results prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = prs.p , BETA = prs.beta , SE = prs.se )) } # Best result is: prs.result [ which.max ( prs.result $ R2 ),] quick p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype - read.table ( EUR.height , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) covariate - read.table ( EUR.covariate , header = T ) pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) null.r2 - summary ( lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )])) $ r.squared prs.result - NULL for ( i in p.threshold ){ pheno.prs - merge ( pheno , read.table ( paste0 ( EUR. , i , .profile ), header = T )[, c ( FID , IID , SCORE )], by = c ( FID , IID )) model - summary ( lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )])) model.r2 - model $ r.squared prs.r2 - model.r2 - null.r2 prs.coef - model $ coeff [ SCORE ,] prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = as.numeric ( prs.coef [ 4 ]), BETA = as.numeric ( prs.coef [ 1 ]), SE = as.numeric ( prs.coef [ 2 ]))) } print ( prs.result [ which.max ( prs.result $ R2 ),]) Which p-value threshold generate the \"best\" PRS? 0.2 How much phenotypic variation does the \"best\" PRS explains? 0.04128065","title":"PLINK"},{"location":"plink/#remove-ambiguous-snps","text":"If the base and target data were generated using different genotyping chips and the chromosome strand (+/-) for either is unknown, then it is not possible to match ambiguous SNPs (i.e. those with complementary alleles, either C/G or A/T) across the data sets, because it will be unknown whether the base and target data are referring to the same allele or not. Ambiguous SNPs can be obtained by examining the bim file: awk !( ($5== A $6== T ) || \\ ($5== T $6== A ) || \\ ($5== G $6== C ) || \\ ($5== C $6== G )) {print} \\ EUR.QC.bim EUR.unambig.snp How many ambiguous SNPs were there? There are 17,260 ambiguous SNPs","title":"Remove Ambiguous SNPs"},{"location":"plink/#strand-flipping","text":"In addition, when there are non-ambiguous mismatch in allele coding between the data sets, such as A/C in the base and G/T in the target data, then this can be resolved by \u2018flipping\u2019 the alleles in the target data to their complementary alleles. This has to be done with mulitpe steps Get the correct A1 alleles for the bim file Without data.table bim - read.table ( EUR.QC.bim , header = F , stringsAsFactors = F ) colnames ( bim ) - c ( CHR , SNP , CM , BP , B.A1 , B.A2 ) height - read.table ( gzfile ( Height.QC.gz ), header = T , stringsAsFactors = F ) # Change all alleles to upper case for easy comparison height $ A1 - toupper ( height $ A1 ) height $ A2 - toupper ( height $ A2 ) bim $ B.A1 - toupper ( bim $ B.A1 ) bim $ B.A2 - toupper ( bim $ B.A2 ) info - merge ( bim , height , by = c ( SNP , CHR , BP )) # Function for finding the complementary allele complement - function ( x ) { switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Get SNPs that has the same alleles across base and target info.match - subset ( info , A1 == B.A1 A2 == B.A2 ) # Identify SNPs that are complementary between base and target info $ C.A1 - sapply ( info $ B.A1 , complement ) info $ C.A2 - sapply ( info $ B.A2 , complement ) info.complement - subset ( info , A1 == C.A1 A2 == C.A2 ) # Update these allele coding in the bim file bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A1 - sapply ( bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A1 , complement ) bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A2 - sapply ( bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A2 , complement ) # identify SNPs that need flipping info.flip - subset ( info , A1 == B.A2 A2 == B.A1 ) # identify SNPs that need flipping complement info.cflip - subset ( info , A1 == C.A2 A2 == C.A1 ) # Update these allele coding in the bim file com.snps - bim $ SNP %in% info.cflip $ SNP bim [ com.snps ,] $ B.A1 - sapply ( bim [ com.snps ,] $ B.A1 , complement ) bim [ com.snps ,] $ B.A2 - sapply ( bim [ com.snps ,] $ B.A2 , complement ) # Get list of SNPs that need to change the A1 encoding flip - rbind ( info.flip , info.cflip ) flip.snp - data.frame ( SNP = flip $ SNP , A1 = flip $ A1 ) write.table ( flip.snp , EUR.update.a1 , quote = F , row.names = F ) write.table ( bim , EUR.QC.adj.bim , quote = F , row.names = F , col.names = F ) # And we want to remove any SNPs that do not match with the base data mismatch - bim $ SNP [ ! ( bim $ SNP %in% info.match $ SNP | bim $ SNP %in% info.complement $ SNP | bim $ SNP %in% flip $ SNP )] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) With data.table library ( data.table ) bim - fread ( EUR.QC.bim ) bim.col - c ( CHR , SNP , CM , BP , B.A1 , B.A2 ) setnames ( bim , colnames ( bim ), bim.col ) height - fread ( Height.QC.gz ) # Change all alleles to upper case for easy comparison height [, c ( A1 , A2 ) := list ( toupper ( A1 ), toupper ( A2 ))] bim [, c ( B.A1 , B.A2 ) := list ( toupper ( B.A1 ), toupper ( B.A2 ))] info - merge ( bim , height , by = c ( SNP , CHR , BP )) # Function for calculating the complementary allele complement - function ( x ){ switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Identify SNPs that are complementary between base and target com.snps - info [ sapply ( B.A1 , complement ) == A1 sapply ( B.A2 , complement ) == A2 , SNP ] # Now update the bim file bim [ SNP %in% com.snps , c ( B.A1 , B.A2 ) := list ( sapply ( B.A1 , complement ), sapply ( B.A2 , complement ))] # identify SNPs that need flipping complement com.flip - info [ sapply ( B.A1 , complement ) == A2 sapply ( B.A2 , complement ) == A1 , SNP ] # Now update the bim file bim [ SNP %in% com.flip , c ( B.A1 , B.A2 ) := list ( sapply ( B.A1 , complement ), sapply ( B.A2 , complement ))] # Obtain list of SNPs that require flipping flip - info [ B.A1 == A2 B.A2 == A1 ] # Now generate file for PLINK fwrite ( flip [, c ( SNP , A1 )], EUR.update.a1 , sep = \\t ) # Write the updated bim file fwrite ( bim , EUR.QC.adj.bim , col.names = F , sep = \\t ) # We can then remove all mismatch SNPs matched - info [( A1 == B.A1 A2 == B.A2 ) | ( A1 == B.A2 A2 == B.A1 ) | ( A1 == sapply ( B.A1 , complement ) A2 == sapply ( B.A2 , complement )) | ( A1 == sapply ( B.A2 , complement ) A2 == sapply ( B.A1 , complement ))] mismatch - bim [ ! SNP %in% matched $ SNP , SNP ] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) The above script will generate three files: EUR.QC.adj.bim , EUR.update.a1 and EUR.mismatch . We want to replace EUR.QC.bim with EUR.QC.adj.bim : # Make a back up mv EUR.QC.bim EUR.QC.bim.bk ln -s EUR.QC.adj.bim EUR.QC.bim We can then generate a new genotype file with the correct genetic encodings plink \\ --bfile EUR.QC \\ --a1-allele EUR.update.a1 \\ --make-bed \\ --keep EUR.valid.sample \\ --extract EUR.unambig.snp \\ --exclude EUR.mismatch \\ --out EUR.QC.flipped","title":"Strand Flipping"},{"location":"plink/#update-effect-size","text":"When odd ratios (OR) instead of BETA are provided, the PRS might have to calculated using a multiplicative model. To simplify the calculation, we usually take the natural logarithm of the OR such that an additive model can be applied. We can obtain the transformed summary statistics with R : Without data.table dat - read.table ( gzfile ( Height.QC.gz ), header = T ) dat $ OR - log ( dat $ OR ) write.table ( dat , Height.QC.Transformed , quote = F , row.names = F ) With data.table library ( data.table ) dat - fread ( Height.QC.gz ) fwrite ( dat [, OR := log ( OR )], Height.QC.Transformed , sep = \\t ) Warning It might be tempting to perform the log transofrmation using awk . However, due to a lower arithmetic precision of awk , less accurate results might be obtained. Therefore it is best to do the transformation in R or allow the PRS software to perform the transformation directly.","title":"Update Effect Size"},{"location":"plink/#clumping","text":"Linkage disequilibrium introduce a strong correlation structure across the genome, makes identifying the independent genetic effects extremely challenging. One simple method is to perform clumping, which preferentially selects SNPs most associated with the trait under study when removing SNPs in LD. plink \\ --bfile EUR.QC.flipped \\ --clump-p1 1 \\ --clump-r2 0 .1 \\ --clump-kb 250 \\ --clump Height.QC.transformed \\ --clump-snp-field SNP \\ --clump-field P \\ --out EUR Each of the new parameters corresponds to the following Paramter Value Description clump-p1 1 P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping clump-r2 0.1 SNPs having \\(r^2\\) higher than 0.1 with the index SNPs will be removed clump-kb 250 SNPs within 250k of the index SNP are considered for clumping clump Height.QC.transformed Summary statistic file containing the p-value information clump-snp-field SNP Specify that the column SNP contains the SNP IDs clump-field P Specify that the column P contains the P-value information A more detailed document can be found here Note The \\(r^2\\) values computed by --clump are based on maximum likelihood haplotype frequency estimates This will generate EUR.clumped , containing the index SNPs after clumping is performed. We can extract the index SNP ID by doing awk NR!=1{print $3} EUR.clumped EUR.valid.snp $3 because the third column contains the SNP ID Note If your target sample is small (e.g. 500), you can try using the 1000 Genome samples for the LD calculation. Make sure you use the population that best represents your sample.","title":"Clumping"},{"location":"plink/#generate-prs","text":"plink provide a handy function --score and --q-score-range for calculating polygenic score. We will need three files The summary statistic file: Height.QC.Transformed A file containing SNP ID and their corresponding p-value ( $1 because SNP ID is located at the first column; $8 because P-value is located at the eigth column) awk {print $1,$8} Height.QC.Transformed SNP.pvalue A file containing the P-value thresholds we are testing. Here we will only test a few for illustration purposes echo 0.001 0 0.001 range_list echo 0.05 0 0.05 range_list echo 0.1 0 0.1 range_list echo 0.2 0 0.2 range_list echo 0.3 0 0.3 range_list echo 0.4 0 0.4 range_list echo 0.5 0 0.5 range_list The format of the range_list file should be as follow Name of Threshold Lower bound Upper Bound Note The boundary are inclusive. For example, for the 0.05 threshold, we include all SNPs with P-value from 0 to 0.05 , including any SNPs with P-value equal to 0.05 We can then calculate the PRS with the following plink command: plink \\ --bfile EUR.QC.flipped \\ --extract EUR.valid.snp \\ --score Height.QC.Transformed 1 4 11 header \\ --q-score-range range_list SNP.pvalue \\ --out EUR Meaning of the new parameters are as follow Paramter Value Description score Height.QC.Transformed 1 4 11 header We read from the Height.QC.Transformed file, assuming the 1 st column to be the SNP ID; 4 th column to be the effective allele information; 11 th column to be the effect size estimate; and the file contains a header q-score-range range_test SNP.pvalue We want to calculate PRS based on the thresholds defined in range_test , where the threshold values (p-values) were stored in SNP.pvalue The above command and range_list will generate 7 files: EUR.0.5.profile EUR.0.4.profile EUR.0.3.profile EUR.0.2.profile EUR.0.1.profile EUR.0.05.profile EUR.0.001.profile Note The default formular for PRS calculation in PLINK is: (Assuming the effect size of SNP \\(i\\) is \\(S_i\\) ; the number of effective allele observed in sample \\(j\\) is \\(G_{ij}\\) ; the ploidy of the sample is \\(P\\) (It should be 2 for human); the number of samples included in the PRS be \\(N\\) ; and the number of non-missing SNPs observed in sample \\(j\\) be \\(M_j\\) ) $$ PRS_j =\\frac{ \\sum_i^NS_i*G_{ij}}{P*M_j} $$ If sample has a missing genotype for SNP \\(i\\) , the population minor allele frequency times ploidy ( \\(MAF_i*P\\) ) is used inplace of \\(G_{ij}\\)","title":"Generate PRS"},{"location":"plink/#accounting-for-population-stratification","text":"Population structure is the principal source of confounding in GWAS and are usually accounted for by incorporating the principal components (PCs) as a covariate. Similarly, we can incorporate PCs in our PRS analysis to account for population stratification. Again, we can calculate the PCs using plink # First, we need to perform prunning plink \\ --bfile EUR.QC.flipped \\ --extract EUR.valid.snp \\ --indep-pairwise 200 50 0 .25 \\ --out EUR # Then we calculate the first 6 PCs plink \\ --bfile EUR.QC.flipped \\ --extract EUR.prune.in \\ --pca 6 \\ --out EUR Note One way to select the appropriate number of PCs is to perform GWAS on the trait of interest with different number of PCs. LDSC analysis can then be performed on each of the resulted GWAS summary statistics. By observing the estimated intercept, one can select the number of PCs that provide an intercept estimate closer to 1, which might suggest a smaller influence of population stratification. The eigen-vector (PCs) are stored in EUR.eigenvec and can be used as a covariate in the regression model to account for population stratification. Important If the base and target samples are collected from different population (e.g. Caucasian vs African ), the results from PRS analysis will be biased (see Martin et al ).","title":"Accounting for Population Stratification"},{"location":"plink/#finding-the-best-p-value-threshold","text":"The \"best\" p-value threshold for PRS construction are usually not known. To identify the \"best\" PRS, we can perform a regression between the calculated PRS and the sample phenotype and select the PRS that explains most of the phenotypic variation. This can be achieved using R . detail p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) # Read in the phenotype file phenotype - read.table ( EUR.height , header = T ) # Read in the PCs pcs - read.table ( EUR.eigenvec , header = F ) # The default output from plink does not include a header # To make things simple, we will add the appropriate headers # (1:6 because there are 6 PCs) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # Read in the covariates (here, it is sex) covariate - read.table ( EUR.covariate , header = T ) # Now merge the files pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) # We can then calculate the null model (model with PRS) using a linear regression # (as height is quantitative) null.model - lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )]) # And the R2 of the null model is null.r2 - summary ( null.model ) $ r.squared prs.result - NULL for ( i in p.threshold ){ # Go through each p-value threshold prs - read.table ( paste0 ( EUR. , i , .profile ), header = T ) # Merge the prs with the phenotype matrix # We only want the FID, IID and PRS from the PRS file, therefore we only select the # relevant columns pheno.prs - merge ( pheno , prs [, c ( FID , IID , SCORE )], by = c ( FID , IID )) # Now perform a linear regression on Height with PRS and the covariates # ignoring the FID and IID from our model model - lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )]) # model R2 is obtained as model.r2 - summary ( model ) $ r.squared # R2 of PRS is simply calculated as the model R2 minus the null R2 prs.r2 - model.r2 - null.r2 # We can also obtain the coeffcient and p-value of association of PRS as follow prs.coef - summary ( model ) $ coeff [ SCORE ,] prs.beta - as.numeric ( prs.coef [ 1 ]) prs.se - as.numeric ( prs.coef [ 2 ]) prs.p - as.numeric ( prs.coef [ 4 ]) # We can then store the results prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = prs.p , BETA = prs.beta , SE = prs.se )) } # Best result is: prs.result [ which.max ( prs.result $ R2 ),] quick p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype - read.table ( EUR.height , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) covariate - read.table ( EUR.covariate , header = T ) pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) null.r2 - summary ( lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )])) $ r.squared prs.result - NULL for ( i in p.threshold ){ pheno.prs - merge ( pheno , read.table ( paste0 ( EUR. , i , .profile ), header = T )[, c ( FID , IID , SCORE )], by = c ( FID , IID )) model - summary ( lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )])) model.r2 - model $ r.squared prs.r2 - model.r2 - null.r2 prs.coef - model $ coeff [ SCORE ,] prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = as.numeric ( prs.coef [ 4 ]), BETA = as.numeric ( prs.coef [ 1 ]), SE = as.numeric ( prs.coef [ 2 ]))) } print ( prs.result [ which.max ( prs.result $ R2 ),]) Which p-value threshold generate the \"best\" PRS? 0.2 How much phenotypic variation does the \"best\" PRS explains? 0.04128065","title":"Finding the \"Best\" P-value threshold"},{"location":"plink_visual/","text":"Plotting the Results The P-value threshold results can be visualized using R Note We will be using prs.result generated in previous section Without ggplot2 # We strongly recommend the use of ggplot2. Only follow this code if you # are desperate. # Specify that we want to generate plot in EUR.height.bar.png png ( EUR.height.bar.png , height = 10 , width = 10 , res = 300 , unit = in ) # First, obtain the colorings based on the p-value col - suppressWarnings ( colorRampPalette ( c ( dodgerblue , firebrick ))) # We want the color gradient to match the ranking of p-values prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] prs.result $ color - col ( nrow ( prs.result )) prs.result - prs.result [ order ( prs.result $ Threshold ),] # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Generate the axis labels xlab - expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ])) ylab - expression ( paste ( PRS model fit: , R ^ 2 )) # Setup the drawing area layout ( t ( 1 : 2 ), widths = c ( 8.8 , 1.2 )) par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , oma = c ( 0 , 0.5 , 0 , 0 ), mar = c ( 4 , 6 , 0.5 , 0.5 )) # Plotting the bars b - barplot ( height = prs.result $ R2 , col = prs.result $ color , border = NA , ylim = c ( 0 , max ( prs.result $ R2 ) * 1.25 ), axes = F , ann = F ) # Plot the axis labels and axis ticks odd - seq ( 0 , nrow ( prs.result ) +1 , 2 ) even - seq ( 1 , nrow ( prs.result ), 2 ) axis ( side = 1 , at = b [ odd ], labels = prs.result $ Threshold [ odd ], lwd = 2 ) axis ( side = 1 , at = b [ even ], labels = prs.result $ Threshold [ even ], lwd = 2 ) axis ( side = 1 , at = c ( 0 , b [ 1 ], 2 * b [ length ( b )] - b [ length ( b ) -1 ]), labels = c ( , , ), lwd = 2 , lwd.tick = 0 ) # Write the p-value on top of each bar text ( parse ( text = paste ( prs.result $ print.p )), x = b +0.1 , y = prs.result $ R2 + ( max ( prs.result $ R2 ) * 1.05 - max ( prs.result $ R2 )), srt = 45 ) # Now plot the axis lines box ( bty = L , lwd = 2 ) axis ( 2 , las = 2 , lwd = 2 ) # Plot the axis titles title ( ylab = ylab , line = 4 , cex.lab = 1.5 , font = 2 ) title ( xlab = xlab , line = 2.5 , cex.lab = 1.5 , font = 2 ) # Generate plot area for the legend par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , mar = c ( 20 , 0 , 20 , 4 )) prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] image ( 1 , - log10 ( prs.result $ P ), t ( seq_along ( - log10 ( prs.result $ P ))), col = prs.result $ color , axes = F , ann = F ) axis ( 4 , las = 2 , xaxs = r , yaxs = r , tck = 0.2 , col = white ) # plot legend title title ( bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ), ), line = 2 , cex = 1.5 , font = 2 , adj = 0 ) # write the plot to file dev.off () ggplot2 # ggplot2 is a handy package for plotting library ( ggplot2 ) # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Initialize ggplot, requiring the threshold as the x-axis (use factor so that it is uniformly distributed) ggplot ( data = prs.result , aes ( x = factor ( Threshold ), y = R2 )) + # Specify that we want to print p-value on top of the bars geom_text ( aes ( label = paste ( print.p )), vjust = -1.5 , hjust = 0 , angle = 45 , cex = 4 , parse = T ) + # Specify the range of the plot, *1.25 to provide enough space for the p-values scale_y_continuous ( limits = c ( 0 , max ( prs.result $ R2 ) * 1.25 )) + # Specify the axis labels xlab ( expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ]))) + ylab ( expression ( paste ( PRS model fit: , R ^ 2 ))) + # Draw a bar plot geom_bar ( aes ( fill = - log10 ( P )), stat = identity ) + # Specify the colors scale_fill_gradient2 ( low = dodgerblue , high = firebrick , mid = dodgerblue , midpoint = 1e-4 , name = bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ),) ) + # Some beautification of the plot theme_classic () + theme ( axis.title = element_text ( face = bold , size = 18 ), axis.text = element_text ( size = 14 ), legend.title = element_text ( face = bold , size = 18 ), legend.text = element_text ( size = 14 ), axis.text.x = element_text ( angle = 45 , hjust = 1 ) ) # save the plot ggsave ( EUR.height.bar.png , height = 7 , width = 7 ) An example bar plot generated using ggplot2 In addition, we can visualize the relationship between the \"best\" PRS and the phenotype of interest, colored by sex Without ggplot2 # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting plot ( x = dat $ SCORE , y = dat $ Height , col = white , xlab = Polygenic Score , ylab = Height ) with ( subset ( dat , Sex == Male ), points ( x = SCORE , y = Height , col = red )) with ( subset ( dat , Sex == Female ), points ( x = SCORE , y = Height , col = blue )) ggplot2 library ( ggplot2 ) # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting ggplot ( dat , aes ( x = SCORE , y = Height , color = Sex )) + geom_point () + theme_classic () + labs ( x = Polygenic Score , y = Height ) An example scatter plot generated using ggplot2","title":"4. Visualizing PRS"},{"location":"plink_visual/#plotting-the-results","text":"The P-value threshold results can be visualized using R Note We will be using prs.result generated in previous section Without ggplot2 # We strongly recommend the use of ggplot2. Only follow this code if you # are desperate. # Specify that we want to generate plot in EUR.height.bar.png png ( EUR.height.bar.png , height = 10 , width = 10 , res = 300 , unit = in ) # First, obtain the colorings based on the p-value col - suppressWarnings ( colorRampPalette ( c ( dodgerblue , firebrick ))) # We want the color gradient to match the ranking of p-values prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] prs.result $ color - col ( nrow ( prs.result )) prs.result - prs.result [ order ( prs.result $ Threshold ),] # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Generate the axis labels xlab - expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ])) ylab - expression ( paste ( PRS model fit: , R ^ 2 )) # Setup the drawing area layout ( t ( 1 : 2 ), widths = c ( 8.8 , 1.2 )) par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , oma = c ( 0 , 0.5 , 0 , 0 ), mar = c ( 4 , 6 , 0.5 , 0.5 )) # Plotting the bars b - barplot ( height = prs.result $ R2 , col = prs.result $ color , border = NA , ylim = c ( 0 , max ( prs.result $ R2 ) * 1.25 ), axes = F , ann = F ) # Plot the axis labels and axis ticks odd - seq ( 0 , nrow ( prs.result ) +1 , 2 ) even - seq ( 1 , nrow ( prs.result ), 2 ) axis ( side = 1 , at = b [ odd ], labels = prs.result $ Threshold [ odd ], lwd = 2 ) axis ( side = 1 , at = b [ even ], labels = prs.result $ Threshold [ even ], lwd = 2 ) axis ( side = 1 , at = c ( 0 , b [ 1 ], 2 * b [ length ( b )] - b [ length ( b ) -1 ]), labels = c ( , , ), lwd = 2 , lwd.tick = 0 ) # Write the p-value on top of each bar text ( parse ( text = paste ( prs.result $ print.p )), x = b +0.1 , y = prs.result $ R2 + ( max ( prs.result $ R2 ) * 1.05 - max ( prs.result $ R2 )), srt = 45 ) # Now plot the axis lines box ( bty = L , lwd = 2 ) axis ( 2 , las = 2 , lwd = 2 ) # Plot the axis titles title ( ylab = ylab , line = 4 , cex.lab = 1.5 , font = 2 ) title ( xlab = xlab , line = 2.5 , cex.lab = 1.5 , font = 2 ) # Generate plot area for the legend par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , mar = c ( 20 , 0 , 20 , 4 )) prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] image ( 1 , - log10 ( prs.result $ P ), t ( seq_along ( - log10 ( prs.result $ P ))), col = prs.result $ color , axes = F , ann = F ) axis ( 4 , las = 2 , xaxs = r , yaxs = r , tck = 0.2 , col = white ) # plot legend title title ( bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ), ), line = 2 , cex = 1.5 , font = 2 , adj = 0 ) # write the plot to file dev.off () ggplot2 # ggplot2 is a handy package for plotting library ( ggplot2 ) # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Initialize ggplot, requiring the threshold as the x-axis (use factor so that it is uniformly distributed) ggplot ( data = prs.result , aes ( x = factor ( Threshold ), y = R2 )) + # Specify that we want to print p-value on top of the bars geom_text ( aes ( label = paste ( print.p )), vjust = -1.5 , hjust = 0 , angle = 45 , cex = 4 , parse = T ) + # Specify the range of the plot, *1.25 to provide enough space for the p-values scale_y_continuous ( limits = c ( 0 , max ( prs.result $ R2 ) * 1.25 )) + # Specify the axis labels xlab ( expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ]))) + ylab ( expression ( paste ( PRS model fit: , R ^ 2 ))) + # Draw a bar plot geom_bar ( aes ( fill = - log10 ( P )), stat = identity ) + # Specify the colors scale_fill_gradient2 ( low = dodgerblue , high = firebrick , mid = dodgerblue , midpoint = 1e-4 , name = bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ),) ) + # Some beautification of the plot theme_classic () + theme ( axis.title = element_text ( face = bold , size = 18 ), axis.text = element_text ( size = 14 ), legend.title = element_text ( face = bold , size = 18 ), legend.text = element_text ( size = 14 ), axis.text.x = element_text ( angle = 45 , hjust = 1 ) ) # save the plot ggsave ( EUR.height.bar.png , height = 7 , width = 7 ) An example bar plot generated using ggplot2 In addition, we can visualize the relationship between the \"best\" PRS and the phenotype of interest, colored by sex Without ggplot2 # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting plot ( x = dat $ SCORE , y = dat $ Height , col = white , xlab = Polygenic Score , ylab = Height ) with ( subset ( dat , Sex == Male ), points ( x = SCORE , y = Height , col = red )) with ( subset ( dat , Sex == Female ), points ( x = SCORE , y = Height , col = blue )) ggplot2 library ( ggplot2 ) # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting ggplot ( dat , aes ( x = SCORE , y = Height , color = Sex )) + geom_point () + theme_classic () + labs ( x = Polygenic Score , y = Height ) An example scatter plot generated using ggplot2","title":"Plotting the Results"},{"location":"prsice/","text":"An alternative to plink is PRSice-2 , which automates much of the PRS analyses. Assuming we have the following files File Name Description GIANT.height.gz The original summary statistic. PRSice-2 can directly apply INFO and MAF filtering on the summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.valid.sample This file contains the samples that passed all the QC EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples And PRSice-2 , which can be downloaded from Operating System Link Linux 64-bit v2.2.3 OS X 64-bit v2.2.3 Windows 32-bit v2.2.3 Windows 64-bit v2.2.3 In this tutorial, you will only need PRSice.R and PRSice_XXX where XXX is the operation system Running PRS analysis It is simple to run PRSice-2. First, we need a single covariate file. This can be done with R : covariate - read.table ( EUR.covariate , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) cov - merge ( covariate , pcs , by = c ( FID , IID )) write.table ( cov , EUR.cov , quote = F , row.names = F ) which generates EUR.cov PRSice-2 can then be run to obtain the PRS results: Linux Rscript PRSice.R \\ --prsice PRSice_linux \\ --base Height.QC.gz \\ --target EUR.QC \\ --keep EUR.valid.sample \\ --binary-target F \\ --pheno-file EUR.height \\ --cov-file EUR.cov \\ --maf-base MAF,0.05 \\ --info-base INFO,0.8 \\ --out EUR OS X Rscript PRSice.R \\ --prsice PRSice_mac \\ --base Height.QC.gz \\ --target EUR.QC \\ --keep EUR.valid.sample \\ --binary-target F \\ --pheno-file EUR.height \\ --cov-file EUR.cov \\ --maf-base MAF,0.05 \\ --info-base INFO,0.8 \\ --out EUR Windows Rscript PRSice.R ^ --prsice PRSice_win64.exe ^ --base Height.QC.gz ^ --target EUR.QC ^ --keep EUR.valid.sample ^ --binary-target F ^ --pheno-file EUR.height ^ --cov-file EUR.cov ^ --maf-base MAF,0.05 ^ --info-base INFO,0.8 ^ --out EUR This will automatically perform a \"high-resolution scoring\" and generate the \"best\" PRS (in EUR.best ) and relevant graphs","title":"PRSice-2"},{"location":"prsice/#running-prs-analysis","text":"It is simple to run PRSice-2. First, we need a single covariate file. This can be done with R : covariate - read.table ( EUR.covariate , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) cov - merge ( covariate , pcs , by = c ( FID , IID )) write.table ( cov , EUR.cov , quote = F , row.names = F ) which generates EUR.cov PRSice-2 can then be run to obtain the PRS results: Linux Rscript PRSice.R \\ --prsice PRSice_linux \\ --base Height.QC.gz \\ --target EUR.QC \\ --keep EUR.valid.sample \\ --binary-target F \\ --pheno-file EUR.height \\ --cov-file EUR.cov \\ --maf-base MAF,0.05 \\ --info-base INFO,0.8 \\ --out EUR OS X Rscript PRSice.R \\ --prsice PRSice_mac \\ --base Height.QC.gz \\ --target EUR.QC \\ --keep EUR.valid.sample \\ --binary-target F \\ --pheno-file EUR.height \\ --cov-file EUR.cov \\ --maf-base MAF,0.05 \\ --info-base INFO,0.8 \\ --out EUR Windows Rscript PRSice.R ^ --prsice PRSice_win64.exe ^ --base Height.QC.gz ^ --target EUR.QC ^ --keep EUR.valid.sample ^ --binary-target F ^ --pheno-file EUR.height ^ --cov-file EUR.cov ^ --maf-base MAF,0.05 ^ --info-base INFO,0.8 ^ --out EUR This will automatically perform a \"high-resolution scoring\" and generate the \"best\" PRS (in EUR.best ) and relevant graphs","title":"Running PRS analysis"},{"location":"target/","text":"Next, we'd like to perform basic quality controls (QC) on the target genotype data. In this tutorial, we've simulated some samples using the 1000 genome european genotypes. You can download the data here . Or you can download using the following script: curl https://github.com/choishingwan/PRS-Tutorial/raw/master/resources/EUR.zip -L -O Unzip the data as follow: unzip EUR.zip What's the md5sum of the genotype files? File md5sum EUR.bed 96ce8f494a57114eaee6ef9741676f58 EUR.bim 852d54c9b6d1159f89d4aa758869e72a EUR.covariate afff13f8f9e15815f2237a62b8bec00b EUR.fam 8c6463c0d8f32f975cdc423b1b80a951 EUR.height 052beb4cae32ac7673f1d6b9e854c85b Note We assume PLINK is installed in your PATH directory, which allow us to use plink instead of ./plink . If PLINK is not in your PATH directory, replace all instance of plink in the tutorial to ./plink assuming the PLINK executable is located within your working directory Genotype file format Basic filterings The power and validity of PRS analyses are highly dependent on the quality of the base and target data, therefore both data sets must be quality controlled to the high standards implemented in GWAS studies, e.g. removing SNPs with low genotyping rate, low minor allele frequency, violates the Hardy-Weinberg Equilibrium and individuals with low genotyping rate (see Marees et al ). The following plink command perform some basic filterings plink --bfile EUR \\ --maf 0 .05 \\ --hwe 1e-6 \\ --geno 0 .01 \\ --mind 0 .01 \\ --make-bed \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Inform plink that the input genotype files should have a prefix of EUR maf 0.05 Filter out any SNPs with minor allele frequency less than 0.05. Genotype error are more likely to influence SNPs with low MAF. Large sample size can adapt a lower MAF threshold hwe 1e-6 Filtering SNPs with low p-value from the Hardy-Weinberg exact test. SNPs with significant p-value from the HWE test are more likely to harbor genotyping error or are under selection. Filtering should be performed on the control samples to avoid filtering SNPs that are causal (under selection in cases) geno 0.01 Exclude SNPs that are missing in large proportion of subjects. A two pass filtering is usually performed (see Marees et al ). mind 0.01 Exclude individual who have a high rate of genotype missingness. This might indicate problems in the DNA sample. (see Marees et al for more details). make-bed - Inform plink to generate a binary genotype file out EUR Inform plink that all output should have a prefix of EUR How many SNPs were filtered? A total of 266 SNPs were removed due to Hardy-Weinberg exact test results Filter related samples Related samples in the target data might lead to overfitted results, hampering the generalizability of the results. To remove related samples, we first need to perform prunning to remove highly correlated SNPs: plink \\ --bfile EUR.QC \\ --indep-pairwise 200 50 0 .25 \\ --out EUR.QC This will generate two files 1) EUR.QC.prune.in and 2) EUR.QC.prune.out All SNPs within EUR.QC.prune.in has a pairwise \\(r^2 0.25\\) Samples with more than third-degree relatedness ( \\(\\text{pi-hat} 0.125\\) ) can then be removed with plink \\ --bfile EUR.QC \\ --extract EUR.QC.prune.in \\ --rel-cutoff 0 .125 \\ --out EUR.QC Note Normally, we can generate a new genotype file using the new sample list. However, this will use up a lot of storage space. Using plink 's --extract , --exclude , --keep , --remove functions, we can work solely on the list of samples and SNPs without duplicating the genotype file, therefore reducing the storage space usage. Note A greedy algorithm is used to remove the related samples. Which depending on the random seed used, might generate different results. To reproduce the same result, you might need to specify the random seed usage. PLINK's related sample removal does not take into account of the sample phenotype. If one would like to minimize lost of cases for example, a software called GreedyRelated can be used. Remove samples with abnormal heterozygosity rate Individual with high or low heterozygosity rate can be contaminated or are inbreed. It is therefore a good idea to remove these samples from our dataset before continuing the analyse. Heterozygosity rate can be calculated using plink after performing prunning. plink \\ --bfile EUR.QC \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.rel.id \\ --het \\ --out EUR This will generate the EUR.het file which contains the F coefficient estimates. It will be easier to filter the samples using R instead of awk : Open a R section by tying R in your terminal Without library dat - read.table ( EUR.het , header = T ) # Read in the EUR.het file, specify it has header m - mean ( dat $ F ) # Calculate the mean s - sd ( dat $ F ) # Calculate the SD valid - subset ( dat , F = m +3 * s F = m -3 * s ) # Get any samples with F coefficient within 3 SD of the population mean write.table ( valid [, c ( 1 , 2 )], EUR.valid.sample , quote = F , row.names = F ) # print FID and IID for valid samples With data.table library ( data.table ) # Read in file dat - fread ( EUR.het ) # Get samples with F coefficient within 3 SD of the population mean valid - dat [ F = mean ( F ) +3 * sd ( F ) F = mean ( F ) -3 * sd ( F )] # print FID and IID for valid samples fwrite ( valid [, c ( FID , IID )], EUR.valid.sample , sep = \\t ) Check for mis-matched Sex information Note As sex chromosome information are missing from our simulated samples. It is not possible to perform the mis-match sex check. (And we used simulated sex) Thus, this section is only served as a reference in case your samples contain the sex chromosome and sex information which permits checking if there are mis-matched sex information Sometimes, sample mislabeling can occur, which can lead to invalid results. A good indication of mislabeled sample is a mismatch between the biological sex and the reported sex. If the biological sex does not match up with the reported sex, it is likely that the sample has been mislabeled. Before performing sex check, prunning should be performed (see here ). Sex check can then easily be carried out using plink plink \\ --bfile EUR.QC \\ --extract EUR.QC.prune.in \\ --keep EUR.valid.sample \\ --check-sex \\ --out EUR This will generate a file called EUR.sexcheck containing the F-statistics for each individual. For male, the F-statistic should be 0.8 and Female should have a value 0.2. awk NR==FNR{a[$1]=$5} \\ NR!=FNR a[$1]==1 $6 0.8 {print $1,$2} \\ NR!=FNR a[$1]==2 $6 0.2 {print $1,$2} \\ EUR.QC.fam EUR.sexcheck EUR.valid.sex Here is a breakdown of the above script 1. Read in the first file ( NR==FNR ) and store the sex ( $5 ) into a dictionary using the FID ( $1 ) as the key 2. Read in the second file ( NR!=FNR ), if the individual is a male ( a[$1]==1 ) and the F-statistic ( $6 ) is larger than 0.8, print its FID and IID 3. Read in the second file ( NR!=FNR ), if the individual is a female ( a[$1]==2 ) and the F-statistic ( $6 ) is less than 0.2, print its FID and IID The samples can then be extracted using the --keep command.","title":"2. Target Genotype QC"},{"location":"target/#genotype-file-format","text":"","title":"Genotype file format"},{"location":"target/#basic-filterings","text":"The power and validity of PRS analyses are highly dependent on the quality of the base and target data, therefore both data sets must be quality controlled to the high standards implemented in GWAS studies, e.g. removing SNPs with low genotyping rate, low minor allele frequency, violates the Hardy-Weinberg Equilibrium and individuals with low genotyping rate (see Marees et al ). The following plink command perform some basic filterings plink --bfile EUR \\ --maf 0 .05 \\ --hwe 1e-6 \\ --geno 0 .01 \\ --mind 0 .01 \\ --make-bed \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Inform plink that the input genotype files should have a prefix of EUR maf 0.05 Filter out any SNPs with minor allele frequency less than 0.05. Genotype error are more likely to influence SNPs with low MAF. Large sample size can adapt a lower MAF threshold hwe 1e-6 Filtering SNPs with low p-value from the Hardy-Weinberg exact test. SNPs with significant p-value from the HWE test are more likely to harbor genotyping error or are under selection. Filtering should be performed on the control samples to avoid filtering SNPs that are causal (under selection in cases) geno 0.01 Exclude SNPs that are missing in large proportion of subjects. A two pass filtering is usually performed (see Marees et al ). mind 0.01 Exclude individual who have a high rate of genotype missingness. This might indicate problems in the DNA sample. (see Marees et al for more details). make-bed - Inform plink to generate a binary genotype file out EUR Inform plink that all output should have a prefix of EUR How many SNPs were filtered? A total of 266 SNPs were removed due to Hardy-Weinberg exact test results","title":"Basic filterings"},{"location":"target/#filter-related-samples","text":"Related samples in the target data might lead to overfitted results, hampering the generalizability of the results. To remove related samples, we first need to perform prunning to remove highly correlated SNPs: plink \\ --bfile EUR.QC \\ --indep-pairwise 200 50 0 .25 \\ --out EUR.QC This will generate two files 1) EUR.QC.prune.in and 2) EUR.QC.prune.out All SNPs within EUR.QC.prune.in has a pairwise \\(r^2 0.25\\) Samples with more than third-degree relatedness ( \\(\\text{pi-hat} 0.125\\) ) can then be removed with plink \\ --bfile EUR.QC \\ --extract EUR.QC.prune.in \\ --rel-cutoff 0 .125 \\ --out EUR.QC Note Normally, we can generate a new genotype file using the new sample list. However, this will use up a lot of storage space. Using plink 's --extract , --exclude , --keep , --remove functions, we can work solely on the list of samples and SNPs without duplicating the genotype file, therefore reducing the storage space usage. Note A greedy algorithm is used to remove the related samples. Which depending on the random seed used, might generate different results. To reproduce the same result, you might need to specify the random seed usage. PLINK's related sample removal does not take into account of the sample phenotype. If one would like to minimize lost of cases for example, a software called GreedyRelated can be used.","title":"Filter related samples"},{"location":"target/#remove-samples-with-abnormal-heterozygosity-rate","text":"Individual with high or low heterozygosity rate can be contaminated or are inbreed. It is therefore a good idea to remove these samples from our dataset before continuing the analyse. Heterozygosity rate can be calculated using plink after performing prunning. plink \\ --bfile EUR.QC \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.rel.id \\ --het \\ --out EUR This will generate the EUR.het file which contains the F coefficient estimates. It will be easier to filter the samples using R instead of awk : Open a R section by tying R in your terminal Without library dat - read.table ( EUR.het , header = T ) # Read in the EUR.het file, specify it has header m - mean ( dat $ F ) # Calculate the mean s - sd ( dat $ F ) # Calculate the SD valid - subset ( dat , F = m +3 * s F = m -3 * s ) # Get any samples with F coefficient within 3 SD of the population mean write.table ( valid [, c ( 1 , 2 )], EUR.valid.sample , quote = F , row.names = F ) # print FID and IID for valid samples With data.table library ( data.table ) # Read in file dat - fread ( EUR.het ) # Get samples with F coefficient within 3 SD of the population mean valid - dat [ F = mean ( F ) +3 * sd ( F ) F = mean ( F ) -3 * sd ( F )] # print FID and IID for valid samples fwrite ( valid [, c ( FID , IID )], EUR.valid.sample , sep = \\t )","title":"Remove samples with abnormal heterozygosity rate"},{"location":"target/#check-for-mis-matched-sex-information","text":"Note As sex chromosome information are missing from our simulated samples. It is not possible to perform the mis-match sex check. (And we used simulated sex) Thus, this section is only served as a reference in case your samples contain the sex chromosome and sex information which permits checking if there are mis-matched sex information Sometimes, sample mislabeling can occur, which can lead to invalid results. A good indication of mislabeled sample is a mismatch between the biological sex and the reported sex. If the biological sex does not match up with the reported sex, it is likely that the sample has been mislabeled. Before performing sex check, prunning should be performed (see here ). Sex check can then easily be carried out using plink plink \\ --bfile EUR.QC \\ --extract EUR.QC.prune.in \\ --keep EUR.valid.sample \\ --check-sex \\ --out EUR This will generate a file called EUR.sexcheck containing the F-statistics for each individual. For male, the F-statistic should be 0.8 and Female should have a value 0.2. awk NR==FNR{a[$1]=$5} \\ NR!=FNR a[$1]==1 $6 0.8 {print $1,$2} \\ NR!=FNR a[$1]==2 $6 0.2 {print $1,$2} \\ EUR.QC.fam EUR.sexcheck EUR.valid.sex Here is a breakdown of the above script 1. Read in the first file ( NR==FNR ) and store the sex ( $5 ) into a dictionary using the FID ( $1 ) as the key 2. Read in the second file ( NR!=FNR ), if the individual is a male ( a[$1]==1 ) and the F-statistic ( $6 ) is larger than 0.8, print its FID and IID 3. Read in the second file ( NR!=FNR ), if the individual is a female ( a[$1]==2 ) and the F-statistic ( $6 ) is less than 0.2, print its FID and IID The samples can then be extracted using the --keep command.","title":"Check for mis-matched Sex information"}]}